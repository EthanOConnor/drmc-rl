# SMDP-PPO configuration for placement policy training

# Algorithm selection
algo: ppo_smdp

# Training hyperparameters
train:
  total_steps: 500000000
  checkpoint_interval: 100000

# SMDP-PPO specific
smdp_ppo:
  # Learning
  lr: 3.0e-4
  gamma: 0.997
  gae_lambda: 0.95
  
  # PPO parameters
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  
  # Rollout
  decisions_per_update: 512
  num_epochs: 4
  minibatch_size: 128
  
  # Policy head architecture
  # Options: "dense" (recommended), "shift_score", "factorized"
  head_type: "dense"
  pill_embed_dim: 128
  # Extra 64-channel residual blocks in the board encoder (0 keeps the tiny baseline).
  encoder_blocks: 32

  # Optional aux vector inputs (derived from obs + info).
  #   - none: board + pill colors only (legacy)
  #   - v1: adds speed/virus/level/time/height/progress scalars
  aux_spec: "v1"
  
  # Exploration
  entropy_schedule_end: 0.005
  entropy_schedule_steps: 100000000
  use_gumbel_topk: false
  gumbel_k: 2
  
  # Loss
  value_loss_type: "huber"  # "mse" or "huber"

# Environment
env:
  id: "DrMarioPlacementEnv-v0"
  obs_mode: state
  # Default to the in-process C++ pool backend for training throughput.
  # Override on the CLI with `--backend libretro --core quicknes --rom-path ...` for emulator runs.
  backend: cpp-pool
  # Observation representation for state-mode:
  #   - extended: 16ch (explicit virus/static/falling + scalars)
  #   - bitplane_bottle: 4ch (bottle-only color planes + virus_mask)
  #   - bitplane_bottle_mask: 8ch (bottle-only + feasibility planes injected by placement env)
  #   - bitplane: 12ch (type-blind colors + masks + scalars)
  #   - bitplane_reduced: 6ch (colors + virus + falling + preview)
  #   - bitplane_reduced_mask: 10ch (reduced + feasibility mask planes injected by placement env)
  state_repr: bitplane_bottle_mask
  num_envs: 16
  frame_stack: 1
  randomize_rng: true
  # 0=low, 1=medium, 2=high
  speed_setting: 2

# Scripted curriculum (synthetic negative levels: fewer viruses)
curriculum:
  enabled: true
  # ln hop-back curriculum starts at easy match tasks and tightens earlier
  # levels as we introduce harder ones.
  mode: ln_hop_back
  start_level: -15
  max_level: 0
  # Use a 1-sigma Wilson lower bound for stage passes (less strict than 2-sigma).
  confidence_sigmas: 1.0
  # Use a 2-sigma Wilson lower bound for base-skill "mastery" before enabling time budgets.
  time_budget_mastery_sigmas: 2.0
  # Slow the 1-exp(-k) ramp to avoid requiring near-perfect rates too early.
  pass_ramp_exponent_multiplier: 0.6
  # EMA-based confidence gate (stable under non-stationary learning).
  confidence_ema_half_life_episodes: 256.0
  confidence_min_effective_episodes: 128.0
  # Require at least ~1 PPO rollout batches per stage before it can advance.
  min_stage_decisions: 512
  # Probe threshold for newly introduced levels (required success rate).
  probe_threshold: 0.25
  window_episodes: 100
  min_episodes: 50
  rehearsal_prob: 0.0
  
# Logging
logdir: "runs/smdp_ppo"
video_interval: 10000
log_interval: 100
