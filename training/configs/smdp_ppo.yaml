# SMDP-PPO configuration for placement policy training

# Algorithm selection
algo: ppo_smdp

# Training hyperparameters
train:
  total_steps: 5000000
  checkpoint_interval: 100000

# SMDP-PPO specific
smdp_ppo:
  # Learning
  lr: 3.0e-4
  gamma: 0.995
  gae_lambda: 0.95
  
  # PPO parameters
  clip_epsilon: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  max_grad_norm: 0.5
  
  # Rollout
  decisions_per_update: 512
  num_epochs: 4
  minibatch_size: 128
  
  # Policy head architecture
  # Options: "dense" (recommended), "shift_score", "factorized"
  head_type: "dense"
  pill_embed_dim: 32
  # Extra 64-channel residual blocks in the board encoder (0 keeps the tiny baseline).
  encoder_blocks: 0
  
  # Exploration
  entropy_schedule_end: 0.003
  entropy_schedule_steps: 1000000
  use_gumbel_topk: false
  gumbel_k: 2
  
  # Loss
  value_loss_type: "mse"  # "mse" or "huber"

# Environment
env:
  id: "DrMarioPlacementEnv-v0"
  obs_mode: state
  # Default to the headless C++ engine for training throughput.
  # Override on the CLI with `--backend libretro --core quicknes --rom-path ...` for emulator runs.
  backend: cpp-engine
  # Observation representation for state-mode:
  #   - extended: 16ch (explicit virus/static/falling + scalars)
  #   - bitplane_bottle: 4ch (bottle-only color planes + virus_mask)
  #   - bitplane_bottle_mask: 8ch (bottle-only + feasibility planes injected by placement env)
  #   - bitplane: 12ch (type-blind colors + masks + scalars)
  #   - bitplane_reduced: 6ch (colors + virus + falling + preview)
  #   - bitplane_reduced_mask: 10ch (reduced + feasibility mask planes injected by placement env)
  state_repr: bitplane_bottle
  num_envs: 16
  frame_stack: 1
  randomize_rng: true

# Scripted curriculum (synthetic negative levels: fewer viruses)
curriculum:
  enabled: true
  start_level: -10
  max_level: 0
  success_threshold: 0.9
  window_episodes: 100
  min_episodes: 50
  rehearsal_prob: 0.1
  
# Logging
logdir: "runs/smdp_ppo"
video_interval: 10000
log_interval: 100
